{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the toal number of enteries in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232799308\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "count=0\n",
    "for i in range(0,500):\n",
    "    num=str(i)\n",
    "    zero_fill_num=num.zfill(5)\n",
    "    with gzip.open('/ichec/work/mucom001c/GoogleSvrLog/clusterdata-2011-1/task_usage/part-'+zero_fill_num+'-of-00500.csv.gz', 'rb') as f:\n",
    "        for line in f:\n",
    "            line=line.decode()\n",
    "            line=line.split(',')\n",
    "            t=line[0]\n",
    "            count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of enteries in the dataset are :1232799308\n",
    "\n",
    "Average entry per file :2465598.6\n",
    "\n",
    "Data attributes in the file :20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding the size of the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39385409295\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import gzip\n",
    "size=0\n",
    "for i in range(0,500):\n",
    "    num=str(i)\n",
    "    zero_fill_num=num.zfill(5)\n",
    "    size+=Path('/ichec/work/mucom001c/GoogleSvrLog/clusterdata-2011-1/task_usage/part-'+zero_fill_num+'-of-00500.csv.gz').stat().st_size\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total size of zipped file is about : 39385409295 byte or 39.386 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def setup_my_environment():\n",
    "    import os\n",
    "    \n",
    "def setenv(var, val):\n",
    "    os.environ[var] = val\n",
    "\n",
    "def prepend_path(var, val):\n",
    "    old_val = os.environ.get(var, '')\n",
    "    os.environ[var] = val + \":\" + old_val\n",
    "    \n",
    "def setup_java():\n",
    "    PKG_ROOT='/ichec/packages/java/8'\n",
    "    setenv('JAVA_PATH', PKG_ROOT)\n",
    "    setenv('JAVA_HOME', PKG_ROOT)\n",
    "    prepend_path('PATH', PKG_ROOT + '/bin')\n",
    "    prepend_path('MANPATH', PKG_ROOT + '/man')\n",
    "    prepend_path('CPATH', PKG_ROOT + '/include')\n",
    "    \n",
    "def setup_spark():\n",
    "    PKG_ROOT='/ichec/packages/spark/2.3.3/kay/spark-2.3.3'\n",
    "    setenv('SPARK_DIST_CLASSPATH', PKG_ROOT + 'spark-2.3.3-bin-kay-spark')\n",
    "    prepend_path('PATH', PKG_ROOT + PKG_ROOT + 'spark-2.3.3-bin-kay-spark/bin')\n",
    "setup_java()\n",
    "setup_spark()\n",
    "setup_my_environment()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    " \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.instances\",\"20\")\\\n",
    "    .config(\"spark.executor.cores\",\"2\")\\\n",
    "    .config(\"spark.worker.instances\",\"20\")\\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "    .config(\"spark.memory.offHeap.size\",\"25g\")\\\n",
    "    .config(\"spark.debug.maxToStringsFields\",\"100\")\\\n",
    "    .appName(\"GoogleSvrLog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/ichec/work/mucom001c/GoogleSvrLog/clusterdata-2011-1/task_usage/*.gz\")\n",
    "df6=df.select('_c0','_c1','_c2','_c3','_c4')\n",
    "df6=df6.selectExpr('_c0 as start_time','_c1 as end_time','_c2 as job_id','_c3 as task_index','_c4 as machine_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+----------+----------+\n",
      "|  start_time|    end_time|    job_id|task_index|machine_id|\n",
      "+------------+------------+----------+----------+----------+\n",
      "|807404000000|807600000000|5822896270|        84|  82731175|\n",
      "|807404000000|807406000000|5912313637|        99|   1269850|\n",
      "|807404000000|807405000000|5912313637|       374| 367886065|\n",
      "|807404000000|807462000000|6320750684|       306|    660385|\n",
      "|807404000000|807600000000|6320750684|       313| 294901680|\n",
      "|807404000000|807600000000|6321705030|         1|4825194215|\n",
      "|807404000000|807600000000|6321705030|       356|  38728345|\n",
      "|807404000000|807600000000|6321705030|       694| 376217870|\n",
      "|807404000000|807600000000|6321705030|       958|4820139430|\n",
      "|807404000000|807600000000|6321910142|       162| 367886065|\n",
      "|807404000000|807600000000|6321910142|       797|    705825|\n",
      "|807404000000|807533000000|6321910142|      1001| 336058605|\n",
      "|807404000000|807600000000|6321910142|      1008|   6608775|\n",
      "|807404000000|807600000000|6321910142|      1067|3349925635|\n",
      "|807404000000|807518000000|6321910142|      1188|1150457725|\n",
      "|807404000000|807576000000|6321910142|      1371|1436484635|\n",
      "|807404000000|807518000000|6321910142|      1935| 329248300|\n",
      "|807404000000|807600000000|6322689242|       458|   6568140|\n",
      "|807404000000|807600000000|6324794741|       564|4802475937|\n",
      "|807404000000|807406000000|6324858588|        19|   2852040|\n",
      "+------------+------------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df5new = df6.withColumn('start_time', from_unixtime(df6.start_time, \"HH\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----------+----------+\n",
      "|start_time|    end_time|    job_id|task_index|machine_id|\n",
      "+----------+------------+----------+----------+----------+\n",
      "|        17|807600000000|5822896270|        84|  82731175|\n",
      "|        17|807406000000|5912313637|        99|   1269850|\n",
      "|        17|807405000000|5912313637|       374| 367886065|\n",
      "|        17|807462000000|6320750684|       306|    660385|\n",
      "|        17|807600000000|6320750684|       313| 294901680|\n",
      "+----------+------------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5new = df5new.withColumn('end_time', from_unixtime(df5new.end_time, \"HH\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+----------+----------+\n",
      "|start_time|end_time|    job_id|task_index|machine_id|\n",
      "+----------+--------+----------+----------+----------+\n",
      "|        17|      05|5822896270|        84|  82731175|\n",
      "|        17|      21|5912313637|        99|   1269850|\n",
      "|        17|      07|5912313637|       374| 367886065|\n",
      "|        17|      01|6320750684|       306|    660385|\n",
      "|        17|      05|6320750684|       313| 294901680|\n",
      "+----------+--------+----------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5new.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|task_index|\n",
      "+----------+\n",
      "|      8304|\n",
      "|     10351|\n",
      "|     15555|\n",
      "|      3414|\n",
      "|      6613|\n",
      "|      7273|\n",
      "|      9586|\n",
      "|     13442|\n",
      "|     15574|\n",
      "|      1090|\n",
      "|      1436|\n",
      "|      1512|\n",
      "|      4937|\n",
      "|     10436|\n",
      "|     13610|\n",
      "|      8433|\n",
      "|      9009|\n",
      "|     14887|\n",
      "|      2294|\n",
      "|      6240|\n",
      "|      9993|\n",
      "|     11078|\n",
      "|     15271|\n",
      "|      1159|\n",
      "|      2904|\n",
      "|       829|\n",
      "|     13772|\n",
      "|     14838|\n",
      "|     14899|\n",
      "|     13865|\n",
      "|      1572|\n",
      "|      3606|\n",
      "|     11332|\n",
      "|      3959|\n",
      "|      6731|\n",
      "+----------+\n",
      "only showing top 35 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5new.select('task_index').distinct().show(35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myproj]",
   "language": "python",
   "name": "conda-env-.conda-myproj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
